# zipco-pipeline

This project implements an **end-to-end data engineering pipeline** for real estate listings.  
It extracts property listing data from an API, loads it into PostgreSQL, transforms it into a **star schema** (fact + dimension tables), and enables **analytics & visualization** using Power BI.


## Project Overview

1. **Extract**  
   - API data is fetched using Python (`requests`).  
   - Results are normalized into tabular format using Pandas.  

2. **Load**  
   - Data is written into a **PostgreSQL database** as the `raw` layer

2. **Transform**  
   - Data is cleaned, column names standardized, and missing values handled.  
   - Business keys such as property type, bedrooms, bathrooms, and office information are normalized.

3. **Load**  
   - Data is written into a **PostgreSQL database**.  
   - A **star schema** is created:
     - Dimension tables (`dim_location`, `dim_office`, `dim_property`, `dim_date`)  
     - Fact table (`fact_listings`)  

4. **Visualize**  
   - The fact and dimension tables can be connected to **Power BI** (or any BI tool) to explore insights:
     - Top offices by number of listings per city  
     - Listings trend over time  
     - Average bedrooms/bathrooms by city  
     - Geographic distribution of listings  

## Repository Structure
```bash
zipco-pipeline/
│
├── etl.py # Python ETL script (Extract, Transform, Load)
├── run_etl.bat # Batch script to trigger the ETL
├── requirements.txt # Python dependencies
├── .gitignore # Git ignore file
├── fact_dimension.sql # SQL script for creating fact and dimension tables
├── insert_tables.sql # SQL inserts for populating dimension/fact tables
├── queries.sql # Example queries for analysis
├── etl.log # Log file generated by ETL runs
└── README.md # Project documentation
```

## Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/zipco-pipeline.git
cd zipco-pipeline
```

2. Create a Virtual Environment and activate
```bash
python -m venv venv
# On Windows (PowerShell)
.\venv\Scripts\Activate.ps1
# On Linux/Mac
source venv/bin/activate
```
3. Install Dependencies
```bash
pip install -r requirements.txt
```

4. Configure Environment Variables
```bash
# Create a .env file in the project root:
API_URL=https://api.example.com/listings
API_KEY=your_api_key
DB_USERNAME=your_db_username
PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_db_name
```

5. Run the ETL
```bash
python etl.py
```
### Database Schema
The project follows a star schema:

#### Dimensions
```bash
dim_location → state, city, postal_code
dim_office → office_name, office_phone
dim_property → property_type, bedrooms, bathrooms
dim_date → calendar date dimension
```
#### Fact Table
```bash
fact_listings → ties to location, office, property, date dimensions
Contains metrics such as full_address, latitude, longitude, listed_date, removed_date
```
6. Power BI Visualization
You can connect Power BI directly to the zipco_production PostgreSQL database:
Build dashboards such as:
Top 5 offices per city
Listings over time (line chart)
Distribution of bedrooms/bathrooms
Map of property listings

#### Tech Stack
```bash
Python → requests, pandas, sqlalchemy, python-dotenv
PostgreSQL → Data warehouse (fact + dimensions)
SQL → Schema + analytic queries
Power BI → Data visualization
```
#### Author
Developed by Rofiat
If you like this project, feel free to ⭐ the repo!